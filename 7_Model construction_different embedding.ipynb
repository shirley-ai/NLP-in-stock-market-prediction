{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本篇代码用于尝试和比较不同Textual representations的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xueli\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\xueli\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\xueli\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\xueli\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\xueli\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\xueli\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\xueli\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import pickle\n",
    "import json\n",
    "tqdm.pandas()\n",
    "pd.options.mode.chained_assignment = None\n",
    "import keras\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from keras.models import Sequential, Model, load_model \n",
    "from keras.layers import Dense, Dropout, GRU,Input, LSTM, Embedding, Bidirectional,SimpleRNN\n",
    "from keras.layers import Flatten, Conv1D, MaxPooling1D, GlobalMaxPooling1D, TimeDistributed, BatchNormalization\n",
    "from keras.layers import concatenate as lconcat\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "#sess_config.gpu_options.allow_growth = True\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.utils import np_utils,plot_model, multi_gpu_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xueli\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用文本数据来做不同的embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"Pickles/processed_data.pkl\")\n",
    "#df = pd.read_csv(\"Data/lemmatized_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_size = 119398\n",
    "#embedding_matrix = np.load(\"Pickles/embedding_matrix.npy\")\n",
    "max_words = 23070\n",
    "embed_dim = 300 \n",
    "aux_shape = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_pad(docs,max_words=max_words):\n",
    "    global t\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(docs)\n",
    "    docs = pad_sequences(sequences = t.texts_to_sequences(docs),maxlen = max_words, padding = 'post')\n",
    "    global vocab_size\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tokenize_and_pad(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用word2vec做词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('word2vec\\\\GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of null word embeddings: 98770\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words_not_found = []\n",
    "\n",
    "embedding_matrix_w2v = np.zeros((vocab_size, embed_dim))\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix_w2v[i] = embedding_vector\n",
    "\n",
    "    except:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix_w2v, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Pickles/embedding_matrix_w2v.npy\",embedding_matrix_w2v)\n",
    "del word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用fasttext做词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, csv, math, codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999995it [04:52, 3424.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 999995 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = codecs.open('fasttext//wiki300.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of null word embeddings: 96060\n"
     ]
    }
   ],
   "source": [
    "#embedding matrix\n",
    "#print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "#nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix_fasttext = np.zeros((vocab_size, embed_dim))\n",
    "for word, i in t.word_index.items():\n",
    "\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix_fasttext[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix_fasttext, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Pickles/embedding_matrix_fasttext.npy\",embedding_matrix_fasttext)\n",
    "del embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与前面的deep learning代码类似，定义一个custom metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# define roc_callback, inspired by https://github.com/keras-team/keras/issues/6050#issuecomment-329996505\n",
    "def auc_roc(y_true, y_pred):\n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.metrics.auc(y_true,y_pred)\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义可以生成不同模型的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(output_classes,architecture,aux_shape=aux_shape,vocab_size=vocab_size,embed_dim=embed_dim,embedding_matrix=embedding_matrix,max_seq_len=max_words):\n",
    "    \n",
    "    with tf.device('/cpu:0'): #在当前的cpu上运行\n",
    "        main_input= Input(shape=(max_seq_len,),name='doc_input') #主要输入为文本，Input的维度是最长的文件max words\n",
    "        main = Embedding(input_dim = vocab_size,\n",
    "                            output_dim = embed_dim,\n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length=max_seq_len, \n",
    "                            trainable=False)(main_input)\n",
    "#设置不同的模型供选择\n",
    "    if architecture == 'mlp': \n",
    "        # Densely Connected Neural Network (Multi-Layer Perceptron)\n",
    "        main = Dense(32, activation='relu')(main)\n",
    "        main = Dropout(0.2)(main)\n",
    "        main = Flatten()(main)\n",
    "    elif architecture == 'cnn':\n",
    "        # 1-D Convolutional Neural Network\n",
    "        main = Conv1D(64, 3, strides=1, padding='same', activation='relu')(main)\n",
    "        #Cuts the size of the output in half, maxing over every 2 inputs\n",
    "        main = MaxPooling1D(pool_size=3)(main)\n",
    "        main = Dropout(0.2)(main)\n",
    "        main = Conv1D(32, 3, strides=1, padding='same', activation='relu')(main)\n",
    "        main = GlobalMaxPooling1D()(main)\n",
    "        #model.add(Dense(output_classes, activation='softmax'))\n",
    "    elif architecture == 'rnn':\n",
    "        # LSTM network\n",
    "        main = SimpleRNN(32)(main)\n",
    "        #main = GRU(64,activation='tanh')(main)\n",
    "        #main = LSTM(32, return_sequences=False, dropout=0.2, recurrent_dropout=0.1)(main)\n",
    "        #main = Bidirectional(LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.1))(main)\n",
    "        main = BatchNormalization()(main)\n",
    "    elif architecture ==\"rnn_cnn\":\n",
    "        main = Conv1D(64, 5, padding='same', activation='relu')(main)\n",
    "        main = MaxPooling1D()(main)\n",
    "        main = Dropout(0.2)(main)\n",
    "        #main = Bidirectional(CuDNNGRU(32,return_sequences=False),merge_mode='concat')(main)\n",
    "        main = SimpleRNN(32,return_sequences=False)(main)\n",
    "        main = BatchNormalization()(main)\n",
    "   \n",
    "    else:\n",
    "        print('Error: Model type not found.')\n",
    "      \n",
    "    #辅助输入为其他控制变量，input维度是X的变量数\n",
    "    auxiliary_input = Input(shape=(aux_shape,), name='aux_input')\n",
    "    x = lconcat([main, auxiliary_input])#把两个输入合并\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    main_output = Dense(output_classes, activation='sigmoid', name='main_output')(x)#最终的输出是3个维度\n",
    "    model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output],name=architecture)#该函数返回一个完整的模型，输入，输出，模型名\n",
    "      \n",
    "        #没有GPU用不了\n",
    "    #sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #model = multi_gpu_model(model)\n",
    "    model.compile(optimizer='adam',loss= 'categorical_crossentropy',metrics=['accuracy',auc_roc])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle(\"Pickles/X_train.pkl\")\n",
    "y_train = pd.read_pickle(\"Pickles/y_train.pkl\")\n",
    "docs_train = np.load(\"Pickles/docs_train.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分别训练两种不同embedding的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = build_model(3,\"cnn\"，embedding_matrix=embedding_matrix_w2v)\n",
    "#cnn = build_model(3,\"cnn\"，embedding_matrix=embedding_matrix_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit([docs_train,X_train],y_train,batch_size=128,epochs=10,verbose=1)\n",
    "#10 epochs\n",
    "cnn.save(\"Data/models/cnn13_w2v.hdf5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 讲训练好的两个新模型拿来测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    score =[]\n",
    "    for i in range(10):\n",
    "        score.append(model.evaluate([docs_test,X_test],y_test,batch_size=128))\n",
    "    score = pd.DataFrame(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_pickle(\"Pickles/X_test.pkl\")\n",
    "y_test = pd.read_pickle(\"Pickles/y_test.pkl\")\n",
    "docs_test = np.load(\"Pickles/docs_test.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\xueli\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "343/343 [==============================] - ETA: 2: - ETA: 34s - 113s 329ms/step\n",
      "343/343 [==============================] - ETA: 1: - ETA: 36s - 126s 366ms/step\n",
      "343/343 [==============================] - ETA: 1: - ETA: 24s - 84s 246ms/step\n",
      "343/343 [==============================] - ETA: 1: - ETA: 32s - 107s 313ms/step\n",
      "343/343 [==============================] - ETA: 52 - ETA: 21 - 79s 230ms/step\n",
      "343/343 [==============================] - ETA: 53 - ETA: 21 - 76s 220ms/step\n",
      "343/343 [==============================] - ETA: 1: - ETA: 27s - 95s 277ms/step\n",
      "343/343 [==============================] - ETA: 59 - ETA: 33 - 111s 325ms/step\n",
      "343/343 [==============================] - ETA: 1: - ETA: 27s - 93s 272ms/step\n",
      "343/343 [==============================] - ETA: 1: - ETA: 34s - 114s 333ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn_w2v = load_model(\"Data/models/cnn13_w2v.hdf5\",custom_objects={\"auc_roc\":auc_roc})\n",
    "w2v_test_score = test(cnn_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.111785e+00</td>\n",
       "      <td>0.623907</td>\n",
       "      <td>0.754030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.340556e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.111785e+00</td>\n",
       "      <td>0.623907</td>\n",
       "      <td>0.480639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.111785e+00</td>\n",
       "      <td>0.623907</td>\n",
       "      <td>0.783766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.111785e+00</td>\n",
       "      <td>0.623907</td>\n",
       "      <td>0.784680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.111785e+00</td>\n",
       "      <td>0.623907</td>\n",
       "      <td>0.785021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.111785e+00</td>\n",
       "      <td>0.623907</td>\n",
       "      <td>0.785199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0          1          2\n",
       "count  1.000000e+01  10.000000  10.000000\n",
       "mean   1.111785e+00   0.623907   0.754030\n",
       "std    2.340556e-16   0.000000   0.096065\n",
       "min    1.111785e+00   0.623907   0.480639\n",
       "25%    1.111785e+00   0.623907   0.783766\n",
       "50%    1.111785e+00   0.623907   0.784680\n",
       "75%    1.111785e+00   0.623907   0.785021\n",
       "max    1.111785e+00   0.623907   0.785199"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_test_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_test_score.to_pickle(\"Data//model_performance//w2v_test_score.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\xueli\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "343/343 [==============================] - ETA: 2: - ETA: 36s - 123s 359ms/step\n",
      "343/343 [==============================] - ETA: 57 - ETA: 24 - 90s 261ms/step\n",
      "343/343 [==============================] - ETA: 1: - ETA: 35s - 123s 359ms/step\n",
      "343/343 [==============================] - ETA: 56 - ETA: 25 - 101s 295ms/step\n",
      "343/343 [==============================] - ETA: 1: - ETA: 35s - 126s 367ms/step\n",
      "343/343 [==============================] - ETA: 1: - ETA: 31s - 110s 322ms/step\n",
      "343/343 [==============================] - ETA: 1: - ETA: 29s - 109s 318ms/step\n",
      "343/343 [==============================] - ETA: 1: - ETA: 26s - 101s 295ms/step\n",
      "343/343 [==============================] - ETA: 1: - ETA: 32s - 122s 357ms/step\n",
      "343/343 [==============================] - ETA: 1: - ETA: 35s - 122s 357ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn_fasttext = load_model(\"Data/models/cnn13_fasttext.hdf5\",custom_objects={\"auc_roc\":auc_roc})\n",
    "fasttext_test_score = test(cnn_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.931383</td>\n",
       "      <td>0.641399</td>\n",
       "      <td>0.770336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.931383</td>\n",
       "      <td>0.641399</td>\n",
       "      <td>0.492809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.931383</td>\n",
       "      <td>0.641399</td>\n",
       "      <td>0.800652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.931383</td>\n",
       "      <td>0.641399</td>\n",
       "      <td>0.801398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.931383</td>\n",
       "      <td>0.641399</td>\n",
       "      <td>0.801674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.931383</td>\n",
       "      <td>0.641399</td>\n",
       "      <td>0.801817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0          1          2\n",
       "count  10.000000  10.000000  10.000000\n",
       "mean    0.931383   0.641399   0.770336\n",
       "std     0.000000   0.000000   0.097516\n",
       "min     0.931383   0.641399   0.492809\n",
       "25%     0.931383   0.641399   0.800652\n",
       "50%     0.931383   0.641399   0.801398\n",
       "75%     0.931383   0.641399   0.801674\n",
       "max     0.931383   0.641399   0.801817"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_test_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_test_score.to_pickle(\"Data//model_performance//fasttext_test_score.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9553"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del cnn_fasttext\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
